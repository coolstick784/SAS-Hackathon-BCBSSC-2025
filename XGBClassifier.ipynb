{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30270b81-b032-40b5-be60-13158133cffc",
   "metadata": {},
   "source": [
    "# Predicting Services from Diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6342491-05a3-4174-9673-62f708d7a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from catboost import CatBoostClassifier # For the classifier \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from tqdm import tqdm\n",
    "from contextlib import contextmanager\n",
    "import joblib\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, recall_score,  average_precision_score, make_scorer,  precision_recall_curve, auc\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Suppress warnings to clean output\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c06fb6b-bcac-4135-891c-18cb345dd21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our pre-split training and validation sets.\n",
    "# The dataset looks like this: Each member has their own row, with an ID, ~300 diagnosis codes, and ~300 service codes\n",
    "train_df = pd.read_csv(r\"K:\\Mktcare\\DATA_INTEGRATION_AND_ANALYSIS\\Ben\\Hackathon\\FormattedData\\FilteredMasked\\AllRecords\\ALL_MMI_train.csv\")\n",
    "val_df = pd.read_csv(r\"K:\\Mktcare\\DATA_INTEGRATION_AND_ANALYSIS\\Ben\\Hackathon\\FormattedData\\FilteredMasked\\AllRecords\\ALL_MMI_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2409008c-2675-4adc-a406-c6bed9b97c16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Identify diagnosis and service columns\n",
    "diagnosis_cols = [col for col in train_df.columns if col.startswith('Diagnosis')]\n",
    "service_cols = [col for col in train_df.columns if col.startswith('Service')]\n",
    "\n",
    "# Combine diagnosis and service columns into sets per row\n",
    "train_df['all_diagnoses'] = train_df[diagnosis_cols].apply(lambda row: set(row.dropna()), axis=1)\n",
    "val_df['all_diagnoses'] = val_df[diagnosis_cols].apply(lambda row: set(row.dropna()), axis=1)\n",
    "\n",
    "# Combine service columns into sets of strings per row\n",
    "train_df['all_services'] = train_df[service_cols].apply(lambda row: set(row.dropna().astype(str)), axis=1)\n",
    "val_df['all_services'] = val_df[service_cols].apply(lambda row: set(row.dropna().astype(str)), axis=1)\n",
    "\n",
    "\n",
    "# Fit MultiLabelBinarizer on training diagnoses\n",
    "mlb_diagnosis = MultiLabelBinarizer()\n",
    "X_train = pd.DataFrame(mlb_diagnosis.fit_transform(train_df['all_diagnoses']),\n",
    "                       columns=[f'Diagnosis_{code}' for code in mlb_diagnosis.classes_],\n",
    "                       index=train_df.index)\n",
    "\n",
    "X_val = pd.DataFrame(mlb_diagnosis.transform(val_df['all_diagnoses']),\n",
    "                     columns=[f'Diagnosis_{code}' for code in mlb_diagnosis.classes_],\n",
    "                     index=val_df.index)\n",
    "\n",
    "# Fit MultiLabelBinarizer on training services\n",
    "mlb_service = MultiLabelBinarizer()\n",
    "y_train = pd.DataFrame(mlb_service.fit_transform(train_df['all_services']),\n",
    "                       columns=[f'Service_{code}' for code in mlb_service.classes_],\n",
    "                       index=train_df.index)\n",
    "\n",
    "y_val = pd.DataFrame(mlb_service.transform(val_df['all_services']),\n",
    "                     columns=[f'Service_{code}' for code in mlb_service.classes_],\n",
    "                     index=val_df.index)\n",
    "# Align X_val to X_train's columns\n",
    "X_val = X_val.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# Align y_val to y_train's columns\n",
    "y_val = y_val.reindex(columns=y_train.columns, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a94add9-4f07-4aed-ac0e-948f767afc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered X_train shape: (14209, 421)\n",
      "Filtered X_val shape: (3552, 421)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Read column names from a text file and format them with a prefix\n",
    "# This long_term_codes file contains a list of diagnosis codes that are considered to be long-term. These are the ones we can use to predict services\n",
    "with open('long_term_codes.txt', 'r') as file:\n",
    "    valid_columns = [\n",
    "        \"Diagnosis_\" + line.strip().replace(\" \", \"_\")\n",
    "        for line in file if line.strip()\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "X_train_filtered = X_train[valid_columns]\n",
    "X_val_filtered = X_val[valid_columns]\n",
    "\n",
    "# Display the shape of the filtered datasets\n",
    "print(\"Filtered X_train shape:\", X_train_filtered.shape)\n",
    "print(\"Filtered X_val shape:\", X_val_filtered.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50878f41-2537-4d8e-a1e7-e1c378473152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service_Colonoscopy                0.038356\n",
      "Service_EGD                        0.015624\n",
      "Service_Gallbladder_surgery        0.002674\n",
      "Service_CT_scan                    0.045394\n",
      "Service_Infusions_drugs            0.160673\n",
      "Service_Fentanyl                   0.024984\n",
      "Service_Orthopedic_surgery         0.023788\n",
      "Service_ENT_Respiratory_surgery    0.009923\n",
      "Service_Electrocardiogram          0.115701\n",
      "Service_Hospital_Observation       0.049968\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Assuming y_train is already defined as a DataFrame\n",
    "# Define the mapping of categories to their associated CPT code columns to group service codes. These are the 10 services we are looking to predict \n",
    "# We found these 10 services after manually looking at the data and discussing with experts in the field\n",
    "\n",
    "category_mapping = {\n",
    "    'Service_Colonoscopy': ['Service_45380', 'Service_45378', 'Service_45385'],\n",
    "    'Service_EGD': ['Service_43239'],\n",
    "    'Service_Gallbladder_surgery': ['Service_47562'],\n",
    "    'Service_CT_scan': ['Service_74177', 'Service_74176', 'Service_74178'],\n",
    "    'Service_Infusions_drugs': [\n",
    "        'Service_96372', 'Service_96374', 'Service_96375', 'Service_96361',\n",
    "        'Service_96365', 'Service_96360', 'Service_96376', 'Service_96366',\n",
    "        'Service_J0702', 'Service_J3490', 'Service_J3301',\n",
    "        'Service_J0690', 'Service_J1170', 'Service_J2270', 'Service_J2001'\n",
    "    ],\n",
    "    'Service_Fentanyl':['Service_J3010'],\n",
    "    # 'Service_Ceftriaxone':['Service_J0696'],\n",
    "    'Service_Orthopedic_surgery': ['Service_20610', 'Service_20550', 'Service_29125', 'Service_29075'],\n",
    "    'Service_ENT_Respiratory_surgery': ['Service_31231', 'Service_31575'],\n",
    "    'Service_Electrocardiogram': ['Service_93000', 'Service_93010', 'Service_93005', 'Service_93017', 'Service_93018', 'Service_93015', 'Service_93016'],\n",
    "    'Service_Hospital_Observation': ['Service_99238', 'Service_99232', 'Service_99239', 'Service_99223', 'Service_99233', 'Service_99222', 'Service_99231', 'Service_99221', 'Service_99217', 'Service_99220']\n",
    "}\n",
    "\n",
    "# Create a new DataFrame with category columns\n",
    "category_df = pd.DataFrame()\n",
    "category_df_val = pd.DataFrame()\n",
    "# For each category, set the value to 1 if any associated column has a 1\n",
    "for category, columns in category_mapping.items():\n",
    "    category_df[category] = (y_train[columns].sum(axis=1) > 0).astype(int)\n",
    "    category_df_val[category] = (y_val[columns].sum(axis=1) > 0).astype(int)\n",
    "# Overwrite y_train with the new category DataFrame\n",
    "y_train_filtered = category_df\n",
    "y_val_filtered = category_df_val\n",
    "\n",
    "print(y_train_filtered.sum()/len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c893003e-2f8b-47a0-93c8-afd706df90d7",
   "metadata": {},
   "source": [
    "As we can see, the prevalence of each service is very low compared to the overall population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e79dc0-297d-4efa-aa39-ae0873875159",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Each model is trained on the same input diagnoses because every member could potentially receive any service, but we train separate models because each service is driven by different patterns and therefore needs its own prediction process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23dbce2d-65eb-4d70-99f3-8d3099f2be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"\n",
    "    Context manager to patch joblib to report into tqdm progress bar.\n",
    "    \"\"\"\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65326a98-9b4f-4af5-b18e-7b4162223b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Hyperparameter tuning (XGBoost, micro-AUPRC CV) ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da15c9e9d8f54b5b991ce74a501c5788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Hyperparameter search (CV tasks):   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV micro-AUPRC: 0.324150\n",
      "Best XGB params: {'subsample': 0.6, 'reg_lambda': 5, 'reg_alpha': 0.1, 'n_estimators': 600, 'min_child_weight': 1, 'max_depth': 5, 'learning_rate': 0.03, 'gamma': 2, 'colsample_bytree': 0.6}\n",
      "\n",
      "=== XGBoost (tuned) ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e173767201a498a94bc1b6cfb6f6825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV fold 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b785f68b58e54b4db52ab1c135ed56ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV fold 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fb6d407dec45789a778c7630a1990b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV fold 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost (tuned) micro-AUPRC: 0.324640\n",
      "\n",
      "--- Summary (micro-AUPRC) ---\n",
      "XGBoost (tuned): 0.324640\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train a separate model for each service, given the same diagnoses\n",
    "def train_ovr(model_factory, X, Y, desc=\"Training OVR\"):\n",
    "    n_labels = Y.shape[1]\n",
    "    models = []\n",
    "    for i in tqdm(range(n_labels), desc=desc):\n",
    "        y = Y.iloc[:, i].values if isinstance(Y, pd.DataFrame) else Y[:, i]\n",
    "        clf = model_factory()\n",
    "        clf.fit(X, y)\n",
    "        models.append(clf)\n",
    "    return models\n",
    "\n",
    "# Helper: predict positive class probabilities for OVR models \n",
    "def predict_proba_ovr(models, X):\n",
    "    probs = []\n",
    "    for clf in models:\n",
    "        proba = clf.predict_proba(X)\n",
    "        # take P(y=1)\n",
    "        if proba.ndim == 2 and proba.shape[1] >= 2:\n",
    "            probs.append(proba[:, 1])\n",
    "        else:\n",
    "            probs.append(np.ravel(proba))  # fallback if a single column is returned\n",
    "    return np.vstack(probs).T  # shape: (n_samples, n_labels)\n",
    "\n",
    "# Return predictions after running K-fold cross-validation with k=3\n",
    "def oof_probs_ovr(model_factory, X, Y, n_splits=3, random_state=42):\n",
    "    X_np = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    Y_df = Y if isinstance(Y, pd.DataFrame) else pd.DataFrame(Y)\n",
    "    n_samples, n_labels = X_np.shape[0], Y_df.shape[1]\n",
    "    oof = np.zeros((n_samples, n_labels), dtype=float)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    for fold, (tr_idx, va_idx) in enumerate(kf.split(X_np), start=1):\n",
    "        X_tr, X_va = X_np[tr_idx], X_np[va_idx]\n",
    "        for j in tqdm(range(n_labels), desc=f\"CV fold {fold}\"):\n",
    "            y_tr = Y_df.iloc[tr_idx, j].values\n",
    "            # Handle degenerate fold (only one class)\n",
    "            if np.unique(y_tr).size < 2:\n",
    "                # Use the prevalence in the training fold as a naive probability\n",
    "                p = float(y_tr.mean())\n",
    "                oof[va_idx, j] = p\n",
    "                continue\n",
    "            clf = model_factory()\n",
    "            clf.fit(X_tr, y_tr)\n",
    "            proba = clf.predict_proba(X_va)\n",
    "            proba_pos = proba[:, 1] if proba.ndim == 2 and proba.shape[1] >= 2 else np.ravel(proba)\n",
    "            oof[va_idx, j] = proba_pos\n",
    "    return oof\n",
    "\n",
    "X_val = None\n",
    "y_val = None\n",
    "has_holdout = False\n",
    "\n",
    "\n",
    "\n",
    "# Tune XGB hyperparameters using micro-AUPRC and RandomizedSearchCV\n",
    "# Some classes are much more rare than others, so it is important to use micro-AUPRC rather than macro-AUPRC\n",
    "def tune_xgb_hyperparams(X, Y, n_iter=25, random_state=42):\n",
    "\n",
    "    base_est = XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        tree_method='hist',\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    ovr = OneVsRestClassifier(base_est)\n",
    "    micro_auprc_scorer = make_scorer(average_precision_score, needs_proba=True, average='micro')\n",
    "\n",
    "    param_dist = {\n",
    "        'estimator__n_estimators': [200, 300, 400, 600, 800],\n",
    "        'estimator__learning_rate': [0.01, 0.03, 0.05, 0.07, 0.1],\n",
    "        'estimator__max_depth': [3, 4, 5, 6, 8],\n",
    "        'estimator__min_child_weight': [1, 2, 3, 5, 7, 10],\n",
    "        'estimator__subsample': [0.6, 0.8, 1.0],\n",
    "        'estimator__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'estimator__gamma': [0, 0.5, 1, 2],\n",
    "        'estimator__reg_alpha': [0, 1e-3, 1e-2, 1e-1, 1],\n",
    "        'estimator__reg_lambda': [0.5, 1, 2, 5, 10],\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=ovr,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        scoring=micro_auprc_scorer,\n",
    "        cv=cv,\n",
    "        verbose=0,      # tqdm will handle progress display\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state,\n",
    "        refit=True\n",
    "    )\n",
    "\n",
    "    Y_fit = Y.values if isinstance(Y, pd.DataFrame) else Y\n",
    "\n",
    "    # Each candidate is evaluated across `cv.get_n_splits(X)` splits\n",
    "    total_tasks = n_iter * cv.get_n_splits(X)\n",
    "    with tqdm_joblib(tqdm(total=total_tasks, desc=\"Hyperparameter search (CV tasks)\")):\n",
    "        search.fit(X, Y_fit)\n",
    "\n",
    "    # Extract best XGB params\n",
    "    best_params = {k.replace('estimator__', ''): v\n",
    "                   for k, v in search.best_params_.items()\n",
    "                   if k.startswith('estimator__')}\n",
    "    return best_params, search.best_score_\n",
    "\n",
    "print(\"\\n=== Hyperparameter tuning (XGBoost, micro-AUPRC CV) ===\")\n",
    "best_xgb_params, best_cv_score = tune_xgb_hyperparams(X_train_filtered, y_train_filtered, n_iter=25, random_state=42)\n",
    "print(\"Best CV micro-AUPRC:\", f\"{best_cv_score:.6f}\")\n",
    "print(\"Best XGB params:\", best_xgb_params)\n",
    "\n",
    "# Model factory using the tuned hyperparameters \n",
    "def xgb_factory():\n",
    "    return XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        tree_method='hist',  # use 'gpu_hist' if you have a GPU\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        **best_xgb_params\n",
    "    )\n",
    "\n",
    "# Train & evaluate tuned XGBoost \n",
    "print(f\"\\n=== XGBoost (tuned) ===\")\n",
    "if has_holdout:\n",
    "    # Train OVR models on train, evaluate on holdout\n",
    "    models = train_ovr(xgb_factory, X_train_filtered, y_train_filtered, desc=f\"Training OVR (XGBoost tuned)\")\n",
    "    y_scores = predict_proba_ovr(models, X_val)\n",
    "    # Make sure y_val is array-like (n_samples, n_labels)\n",
    "    y_true = y_val.values if isinstance(y_val, pd.DataFrame) else y_val\n",
    "    micro_auprc = average_precision_score(y_true, y_scores, average='micro')\n",
    "else:\n",
    "    # CV fallback (OOF micro-AUPRC)\n",
    "    y_oof = oof_probs_ovr(xgb_factory, X_train_filtered, y_train_filtered, n_splits=3, random_state=42)\n",
    "    y_true = y_train_filtered.values if isinstance(y_train_filtered, pd.DataFrame) else y_train_filtered\n",
    "    micro_auprc = average_precision_score(y_true, y_oof, average='micro')\n",
    "\n",
    "print(f\"XGBoost (tuned) micro-AUPRC: {micro_auprc:.6f}\")\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"\\n--- Summary (micro-AUPRC) ---\")\n",
    "print(f\"XGBoost (tuned): {micro_auprc:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3d021c-ec1d-47e4-8451-adf7af085c6d",
   "metadata": {},
   "source": [
    "This micro-AUPRC is much higher than the average prevalence, so our model is much better than random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f9ee25-4354-4b21-a9d4-6a9cd957a393",
   "metadata": {},
   "source": [
    "## Save the models, and load them back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05e79b87-0feb-4859-9c28-1b23beedcb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4edd5ec60b3b4299b0a35771643080c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training models:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a list to hold trained models\n",
    "trained_models = []\n",
    "\n",
    "# Show progress bar while training each label\n",
    "for i in tqdm(range(y_train_filtered.shape[1]), desc=\"Training models\"):\n",
    "    model = xgb_factory()\n",
    "    model.fit(X_train_filtered, y_train_filtered.iloc[:, i])\n",
    "\n",
    "    trained_models.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c8287f9-fba2-411e-8ae2-acbb9f6739c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a directory to store models\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "\n",
    "# Save each model individually\n",
    "for i, model in enumerate(trained_models):\n",
    "    joblib.dump(model, f\"saved_models/xgb_model_{i}.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4698e89b-8bcd-45ac-8300-9c80a386d0ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "✅ All models loaded correctly and produce identical predictions.\n"
     ]
    }
   ],
   "source": [
    "# Load models back into a list\n",
    "loaded_models = []\n",
    "for i in range(y_train_filtered.shape[1]):\n",
    "    print(i)\n",
    "    model = joblib.load(f\"saved_models/xgb_model_{i}.pkl\")\n",
    "    loaded_models.append(model)\n",
    "\n",
    "\n",
    "    \n",
    "print(\"✅ All models loaded correctly and produce identical predictions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84d8c08-fc6e-49b5-9b97-39b193153f99",
   "metadata": {},
   "source": [
    "## Compute accuracy, recall, and count for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7619d0d2-322d-4f34-beb0-2629c6fb523c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "                            target  accuracy    recall    n\n",
      "0      Service_Gallbladder_surgery  0.996622  0.333333   12\n",
      "1  Service_ENT_Respiratory_surgery  0.991273  0.074074   27\n",
      "2                      Service_EGD  0.985642  0.303571   56\n",
      "3       Service_Orthopedic_surgery  0.976070  0.024096   83\n",
      "4                 Service_Fentanyl  0.971847  0.040000  100\n",
      "5              Service_Colonoscopy  0.966779  0.376712  146\n",
      "6     Service_Hospital_Observation  0.954955  0.018868  159\n",
      "7                  Service_CT_scan  0.950169  0.027933  179\n",
      "8        Service_Electrocardiogram  0.883727  0.106888  421\n",
      "9          Service_Infusions_drugs  0.829955  0.107143  616\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "for i, model in enumerate(loaded_models):\n",
    "    print(i)\n",
    "    # Get true labels for this target\n",
    "    y_true = y_val_filtered.iloc[:, i] if hasattr(y_val_filtered, \"iloc\") else y_val_filtered[:, i]\n",
    "    \n",
    "    # Predict probabilities or raw predictions\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_val_filtered)[:, 1]  # positive class\n",
    "    else:\n",
    "        y_proba = model.predict(X_val_filtered)\n",
    "    \n",
    "    # Apply threshold\n",
    "    y_pred = (y_proba >= 0.5).astype(int)\n",
    "    \n",
    "    # Compute metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    results.append({\n",
    "        \"target\": y_val_filtered.columns[i] if hasattr(y_val_filtered, \"columns\") else f\"target_{i}\",\n",
    "        \"accuracy\": acc,\n",
    "        \"recall\": rec,\n",
    "        \"n\": sum(y_true)\n",
    "    })\n",
    "\n",
    "# Create DataFrame and sort\n",
    "metrics_df = pd.DataFrame(results)\n",
    "metrics_df = metrics_df.sort_values(by=[\"accuracy\", \"recall\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe0c6ab6-05b6-42f2-8178-7de226f75c98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Service_Gallbladder_surgery</td>\n",
       "      <td>0.996622</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Service_ENT_Respiratory_surgery</td>\n",
       "      <td>0.991273</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Service_EGD</td>\n",
       "      <td>0.985642</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Service_Orthopedic_surgery</td>\n",
       "      <td>0.976070</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Service_Fentanyl</td>\n",
       "      <td>0.971847</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Service_Colonoscopy</td>\n",
       "      <td>0.966779</td>\n",
       "      <td>0.376712</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Service_Hospital_Observation</td>\n",
       "      <td>0.954955</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Service_CT_scan</td>\n",
       "      <td>0.950169</td>\n",
       "      <td>0.027933</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Service_Electrocardiogram</td>\n",
       "      <td>0.883727</td>\n",
       "      <td>0.106888</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Service_Infusions_drugs</td>\n",
       "      <td>0.829955</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            target  accuracy    recall    n\n",
       "0      Service_Gallbladder_surgery  0.996622  0.333333   12\n",
       "1  Service_ENT_Respiratory_surgery  0.991273  0.074074   27\n",
       "2                      Service_EGD  0.985642  0.303571   56\n",
       "3       Service_Orthopedic_surgery  0.976070  0.024096   83\n",
       "4                 Service_Fentanyl  0.971847  0.040000  100\n",
       "5              Service_Colonoscopy  0.966779  0.376712  146\n",
       "6     Service_Hospital_Observation  0.954955  0.018868  159\n",
       "7                  Service_CT_scan  0.950169  0.027933  179\n",
       "8        Service_Electrocardiogram  0.883727  0.106888  421\n",
       "9          Service_Infusions_drugs  0.829955  0.107143  616"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df[metrics_df['n'] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27e98bc6-356d-495d-acac-60f2c193a3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [target, accuracy, recall, n]\n",
       "Index: []"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_low = metrics_df.sort_values(by=[\"recall\", \"n\"], ascending=True).reset_index(drop=True)\n",
    "recall_low[(recall_low['n'] > 10) & (recall_low['recall'] == 0)] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9067c48c-586f-4716-8dee-547dfb657763",
   "metadata": {},
   "source": [
    "It is a given that with only the data we have, it is difficult to fully predict whether a patient would need a service. However, our recall is much higher than a random guess. Our goal is not to predict the necessity of a service perfectly, but rather warn patients who have an increased risk of needing certain services, and this does a great job at doing that for a number of services. None of the services we are predicting have a recall of 0 with a count > 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd1e59-cd0b-4942-8969-38fa852035c0",
   "metadata": {},
   "source": [
    "## Gather more evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33fa71e7-014d-44cc-a997-ad76f4f31168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro AUPRC: 0.334415626058403\n",
      "Macro AUPRC: 0.30130457481599\n",
      "Weighted AUPRC (by positives): 0.32365158836687086\n",
      "\n",
      "Per-Label Metrics (incl. false positives):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_true_list = []\n",
    "y_score_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "results = []\n",
    "\n",
    "# To store FP score series per target\n",
    "fp_scores_by_target = {}\n",
    "# Also store a quick textual snapshot per label\n",
    "fp_top_scores_by_target = {}\n",
    "\n",
    "# Use the index from y_val_filtered if available (for traceability of FP cases)\n",
    "if hasattr('y_val_filtered', '__iter__') and hasattr(y_val_filtered, 'index'):\n",
    "    sample_index = y_val_filtered.index\n",
    "else:\n",
    "    # Fallback: simple integer index\n",
    "    # We'll define it inside the loop when we know the number of rows\n",
    "    sample_index = None\n",
    "\n",
    "for i, model in enumerate(loaded_models):\n",
    "    # True labels for this target\n",
    "    y_true = y_val_filtered.iloc[:, i] if hasattr(y_val_filtered, \"iloc\") else y_val_filtered[:, i]\n",
    "    y_true_arr = np.asarray(y_true).ravel()\n",
    "\n",
    "    # Predicted scores and threshold choice\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_score = model.predict_proba(X_val_filtered)[:, 1]\n",
    "        threshold = 0.5\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        y_score = model.decision_function(X_val_filtered)\n",
    "        threshold = 0.0  # decision_function is typically centered at 0\n",
    "    else:\n",
    "        # Fallback: uses 0/1 predictions as score\n",
    "        y_score = model.predict(X_val_filtered)\n",
    "        threshold = 0.5\n",
    "\n",
    "    y_score_arr = np.asarray(y_score).ravel()\n",
    "\n",
    "    # Build an index if not present yet\n",
    "    if sample_index is None:\n",
    "        sample_index = pd.RangeIndex(start=0, stop=len(y_score_arr), step=1)\n",
    "\n",
    "    # Thresholded predictions (for acc/rec and false positives)\n",
    "    y_pred = (y_score_arr >= threshold).astype(int)\n",
    "    y_pred_list.append(y_pred)\n",
    "\n",
    "    # Accuracy and recall\n",
    "    acc = accuracy_score(y_true_arr, y_pred)\n",
    "    rec = recall_score(y_true_arr, y_pred, zero_division=0)\n",
    "\n",
    "    #False positives (pred=1, true=0) \n",
    "    fp_mask = (y_pred == 1) & (y_true_arr == 0)\n",
    "    fp_scores = y_score_arr[fp_mask]\n",
    "\n",
    "    # Store FP scores as a Series for easy analysis later\n",
    "    label_name = y_val_filtered.columns[i] if hasattr(y_val_filtered, \"columns\") else f\"target_{i}\"\n",
    "    fp_scores_series = pd.Series(fp_scores, index=np.array(sample_index)[fp_mask], name=f\"{label_name}_fp_score\")\n",
    "    fp_scores_by_target[label_name] = fp_scores_series\n",
    "\n",
    "    # Small textual summary (top-5 highest FP scores)\n",
    "    if fp_scores.size > 0:\n",
    "        # Sort descending by score for top offenders\n",
    "        fp_sorted_idx = np.array(sample_index)[fp_mask][np.argsort(-fp_scores)]\n",
    "        top_k = min(5, fp_scores.size)\n",
    "        top_scores = fp_scores[np.argsort(-fp_scores)[:top_k]]\n",
    "        top_ids = fp_sorted_idx[:top_k]\n",
    "        fp_top_scores_by_target[label_name] = list(zip(top_ids, top_scores))\n",
    "        fp_mean = float(np.mean(fp_scores))\n",
    "        fp_median = float(np.median(fp_scores))\n",
    "    else:\n",
    "        fp_top_scores_by_target[label_name] = []\n",
    "        fp_mean = np.nan\n",
    "        fp_median = np.nan\n",
    "\n",
    "    # True positives\n",
    "    true_positives = int(np.sum((y_pred == 1) & (y_true_arr == 1))) \n",
    "    false_positives = int(np.sum((y_pred == 1) & (y_true_arr == 0)))\n",
    "    prevalence = float(np.sum(y_true_arr == 1)) / float(len(y_true_arr))\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n",
    "    \n",
    "    results.append({\n",
    "        \"target\": label_name,\n",
    "        \"accuracy\": acc,\n",
    "        \"recall\": rec,\n",
    "        \"n_pos\": int(np.sum(y_true_arr)),\n",
    "        \"prevalence\":prevalence,\n",
    "        \"true_positives\": true_positives,  # <-- Added field\n",
    "        \"false_positives\": int(fp_mask.sum()),\n",
    "        \"precision\":precision,\n",
    "        \"fp_score_mean\": fp_mean,\n",
    "        \"fp_score_median\": fp_median\n",
    "    })\n",
    "\n",
    "    y_true_list.append(y_true_arr)\n",
    "    y_score_list.append(y_score_arr)\n",
    "\n",
    "# Stack into (n_samples, n_targets)\n",
    "Y_true = np.column_stack(y_true_list)\n",
    "Y_score = np.column_stack(y_score_list)\n",
    "\n",
    "# Micro AUPRC (global, preferred) \n",
    "micro_auprc = average_precision_score(Y_true, Y_score, average='micro')\n",
    "\n",
    "# Macro and Weighted AUPRC for comparison:\n",
    "macro_auprc = average_precision_score(Y_true, Y_score, average='macro')       # equal weight per target\n",
    "weighted_auprc = average_precision_score(Y_true, Y_score, average='weighted') # weighted by positives in each target\n",
    "\n",
    "# Per-target AP\n",
    "ap_per_target = average_precision_score(Y_true, Y_score, average=None)\n",
    "\n",
    "metrics_df = pd.DataFrame(results).sort_values(by=[\"accuracy\", \"recall\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Align AP to the current sorted order index\n",
    "try:\n",
    "    metrics_df[\"AP\"] = ap_per_target[metrics_df.index]\n",
    "except Exception:\n",
    "    # Safer alignment by label name (in case of shape mismatch)\n",
    "    label_names = [y_val_filtered.columns[i] if hasattr(y_val_filtered, \"columns\") else f\"target_{i}\" for i in range(Y_true.shape[1])]\n",
    "    ap_map = {label_names[i]: ap_per_target[i] for i in range(len(label_names))}\n",
    "    metrics_df[\"AP\"] = metrics_df[\"target\"].map(ap_map)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Micro AUPRC:\", micro_auprc)\n",
    "print(\"Macro AUPRC:\", macro_auprc)\n",
    "print(\"Weighted AUPRC (by positives):\", weighted_auprc)\n",
    "print(\"\\nPer-Label Metrics (incl. false positives):\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1321e357-7c8e-4778-b616-9cc40a12fa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>prevalence</th>\n",
       "      <th>true_positives</th>\n",
       "      <th>false_positives</th>\n",
       "      <th>precision</th>\n",
       "      <th>fp_score_mean</th>\n",
       "      <th>fp_score_median</th>\n",
       "      <th>AP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Service_Gallbladder_surgery</td>\n",
       "      <td>0.996622</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>12</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.559908</td>\n",
       "      <td>0.559758</td>\n",
       "      <td>0.571339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Service_ENT_Respiratory_surgery</td>\n",
       "      <td>0.991273</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>27</td>\n",
       "      <td>0.007601</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.606400</td>\n",
       "      <td>0.580063</td>\n",
       "      <td>0.398081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Service_EGD</td>\n",
       "      <td>0.985642</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>56</td>\n",
       "      <td>0.015766</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.662475</td>\n",
       "      <td>0.640888</td>\n",
       "      <td>0.483883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Service_Orthopedic_surgery</td>\n",
       "      <td>0.976070</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>83</td>\n",
       "      <td>0.023367</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.551802</td>\n",
       "      <td>0.531656</td>\n",
       "      <td>0.232641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Service_Fentanyl</td>\n",
       "      <td>0.971847</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.028153</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.554395</td>\n",
       "      <td>0.542135</td>\n",
       "      <td>0.364427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Service_Colonoscopy</td>\n",
       "      <td>0.966779</td>\n",
       "      <td>0.376712</td>\n",
       "      <td>146</td>\n",
       "      <td>0.041104</td>\n",
       "      <td>55</td>\n",
       "      <td>27</td>\n",
       "      <td>0.670732</td>\n",
       "      <td>0.738224</td>\n",
       "      <td>0.745542</td>\n",
       "      <td>0.163132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Service_Hospital_Observation</td>\n",
       "      <td>0.954955</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>159</td>\n",
       "      <td>0.044764</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.604274</td>\n",
       "      <td>0.539807</td>\n",
       "      <td>0.136945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Service_CT_scan</td>\n",
       "      <td>0.950169</td>\n",
       "      <td>0.027933</td>\n",
       "      <td>179</td>\n",
       "      <td>0.050394</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.695375</td>\n",
       "      <td>0.712577</td>\n",
       "      <td>0.162892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Service_Electrocardiogram</td>\n",
       "      <td>0.883727</td>\n",
       "      <td>0.106888</td>\n",
       "      <td>421</td>\n",
       "      <td>0.118525</td>\n",
       "      <td>45</td>\n",
       "      <td>37</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.619662</td>\n",
       "      <td>0.553613</td>\n",
       "      <td>0.355244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Service_Infusions_drugs</td>\n",
       "      <td>0.829955</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>616</td>\n",
       "      <td>0.173423</td>\n",
       "      <td>66</td>\n",
       "      <td>54</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.619017</td>\n",
       "      <td>0.602906</td>\n",
       "      <td>0.144464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            target  accuracy    recall  n_pos  prevalence  \\\n",
       "0      Service_Gallbladder_surgery  0.996622  0.333333     12    0.003378   \n",
       "1  Service_ENT_Respiratory_surgery  0.991273  0.074074     27    0.007601   \n",
       "2                      Service_EGD  0.985642  0.303571     56    0.015766   \n",
       "3       Service_Orthopedic_surgery  0.976070  0.024096     83    0.023367   \n",
       "4                 Service_Fentanyl  0.971847  0.040000    100    0.028153   \n",
       "5              Service_Colonoscopy  0.966779  0.376712    146    0.041104   \n",
       "6     Service_Hospital_Observation  0.954955  0.018868    159    0.044764   \n",
       "7                  Service_CT_scan  0.950169  0.027933    179    0.050394   \n",
       "8        Service_Electrocardiogram  0.883727  0.106888    421    0.118525   \n",
       "9          Service_Infusions_drugs  0.829955  0.107143    616    0.173423   \n",
       "\n",
       "   true_positives  false_positives  precision  fp_score_mean  fp_score_median  \\\n",
       "0               4                4   0.500000       0.559908         0.559758   \n",
       "1               2                6   0.250000       0.606400         0.580063   \n",
       "2              17               12   0.586207       0.662475         0.640888   \n",
       "3               2                4   0.333333       0.551802         0.531656   \n",
       "4               4                4   0.500000       0.554395         0.542135   \n",
       "5              55               27   0.670732       0.738224         0.745542   \n",
       "6               3                4   0.428571       0.604274         0.539807   \n",
       "7               5                3   0.625000       0.695375         0.712577   \n",
       "8              45               37   0.548780       0.619662         0.553613   \n",
       "9              66               54   0.550000       0.619017         0.602906   \n",
       "\n",
       "         AP  \n",
       "0  0.571339  \n",
       "1  0.398081  \n",
       "2  0.483883  \n",
       "3  0.232641  \n",
       "4  0.364427  \n",
       "5  0.163132  \n",
       "6  0.136945  \n",
       "7  0.162892  \n",
       "8  0.355244  \n",
       "9  0.144464  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff76d0a-d098-4b88-96da-7e462a4e4d37",
   "metadata": {},
   "source": [
    "Given that the prevalence of each service is low, the precision, AUPRC and AP scores show that the model is much better at detecting services than a random guess. Even if some patients do not end up needing the service, it is worth it to have a few extra members get a checkup rather than having a member suffer a serious injury that could have been prevented. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8540da-aaae-4b18-9dd3-7a8ef0320efc",
   "metadata": {},
   "source": [
    "\n",
    "# Association Rule Mining: X -> Y\n",
    "Association rule mining helps us map diagnoses to services in plain English, creating rules that others can understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a826d06-8c53-414c-a984-74132f893ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================\n",
      "X → Y Association Rules (confidence ≥ 0.10)\n",
      "=====================\n",
      "\n",
      "=== Target: Service_Colonoscopy ===\n",
      "  - Diagnosis_K57.30=1 ⇒ Service_Colonoscopy (supp=0.011, conf=0.660, lift=16.961)\n",
      "\n",
      "=== Target: Service_EGD ===\n",
      "  - (no rules found)\n",
      "\n",
      "=== Target: Service_Gallbladder_surgery ===\n",
      "  - (no rules found)\n",
      "\n",
      "=== Target: Service_CT_scan ===\n",
      "  - (no rules found)\n",
      "\n",
      "=== Target: Service_Infusions_drugs ===\n",
      "  - Diagnosis_Z79.899=1 ⇒ Service_Infusions_drugs (supp=0.029, conf=0.369, lift=2.259)\n",
      "  - Diagnosis_E66.9=1 ⇒ Service_Infusions_drugs (supp=0.011, conf=0.300, lift=1.839)\n",
      "  - Diagnosis_K21.9=1 ⇒ Service_Infusions_drugs (supp=0.019, conf=0.299, lift=1.829)\n",
      "  - Diagnosis_I10=1 ⇒ Service_Infusions_drugs (supp=0.031, conf=0.260, lift=1.591)\n",
      "  - Diagnosis_E78.5=1 ⇒ Service_Infusions_drugs (supp=0.015, conf=0.231, lift=1.418)\n",
      "  - Diagnosis_E55.9=1 ⇒ Service_Infusions_drugs (supp=0.013, conf=0.226, lift=1.386)\n",
      "  - Diagnosis_F41.9=1 ⇒ Service_Infusions_drugs (supp=0.016, conf=0.217, lift=1.328)\n",
      "\n",
      "=== Target: Service_Fentanyl ===\n",
      "  - (no rules found)\n",
      "\n",
      "=== Target: Service_Orthopedic_surgery ===\n",
      "  - (no rules found)\n",
      "\n",
      "=== Target: Service_ENT_Respiratory_surgery ===\n",
      "  - (no rules found)\n",
      "\n",
      "=== Target: Service_Electrocardiogram ===\n",
      "  - Diagnosis_I10=1 & Diagnosis_Z79.899=1 ⇒ Service_Electrocardiogram (supp=0.010, conf=0.415, lift=3.567)\n",
      "  - Diagnosis_E78.5=1 & Diagnosis_I10=1 ⇒ Service_Electrocardiogram (supp=0.012, conf=0.363, lift=3.125)\n",
      "  - Diagnosis_I10=1 ⇒ Service_Electrocardiogram (supp=0.038, conf=0.312, lift=2.686)\n",
      "  - Diagnosis_E78.5=1 ⇒ Service_Electrocardiogram (supp=0.019, conf=0.291, lift=2.505)\n",
      "  - Diagnosis_E78.2=1 ⇒ Service_Electrocardiogram (supp=0.011, conf=0.269, lift=2.314)\n",
      "  - Diagnosis_Z79.899=1 ⇒ Service_Electrocardiogram (supp=0.021, conf=0.261, lift=2.247)\n",
      "  - Diagnosis_E55.9=1 ⇒ Service_Electrocardiogram (supp=0.015, conf=0.258, lift=2.216)\n",
      "  - Diagnosis_K21.9=1 ⇒ Service_Electrocardiogram (supp=0.015, conf=0.234, lift=2.011)\n",
      "  - Diagnosis_F41.9=1 ⇒ Service_Electrocardiogram (supp=0.015, conf=0.205, lift=1.760)\n",
      "\n",
      "=== Target: Service_Hospital_Observation ===\n",
      "  - (no rules found)\n",
      "\n",
      "--- Global X → Y rules (top 100) ---\n",
      "Diagnosis_K57.30=1 ⇒ Service_Colonoscopy (supp=0.011, conf=0.660, lift=16.961)\n",
      "Diagnosis_I10=1 & Diagnosis_Z79.899=1 ⇒ Service_Electrocardiogram (supp=0.010, conf=0.415, lift=3.567)\n",
      "Diagnosis_Z79.899=1 ⇒ Service_Infusions_drugs (supp=0.029, conf=0.369, lift=2.259)\n",
      "Diagnosis_E78.5=1 & Diagnosis_I10=1 ⇒ Service_Electrocardiogram (supp=0.012, conf=0.363, lift=3.125)\n",
      "Diagnosis_I10=1 ⇒ Service_Electrocardiogram (supp=0.038, conf=0.312, lift=2.686)\n",
      "Diagnosis_E66.9=1 ⇒ Service_Infusions_drugs (supp=0.011, conf=0.300, lift=1.839)\n",
      "Diagnosis_K21.9=1 ⇒ Service_Infusions_drugs (supp=0.019, conf=0.299, lift=1.829)\n",
      "Diagnosis_E78.5=1 ⇒ Service_Electrocardiogram (supp=0.019, conf=0.291, lift=2.505)\n",
      "Diagnosis_E78.2=1 ⇒ Service_Electrocardiogram (supp=0.011, conf=0.269, lift=2.314)\n",
      "Diagnosis_Z79.899=1 ⇒ Service_Electrocardiogram (supp=0.021, conf=0.261, lift=2.247)\n",
      "Diagnosis_I10=1 ⇒ Service_Infusions_drugs (supp=0.031, conf=0.260, lift=1.591)\n",
      "Diagnosis_E55.9=1 ⇒ Service_Electrocardiogram (supp=0.015, conf=0.258, lift=2.216)\n",
      "Diagnosis_K21.9=1 ⇒ Service_Electrocardiogram (supp=0.015, conf=0.234, lift=2.011)\n",
      "Diagnosis_E78.5=1 ⇒ Service_Infusions_drugs (supp=0.015, conf=0.231, lift=1.418)\n",
      "Diagnosis_E55.9=1 ⇒ Service_Infusions_drugs (supp=0.013, conf=0.226, lift=1.386)\n",
      "Diagnosis_F41.9=1 ⇒ Service_Infusions_drugs (supp=0.016, conf=0.217, lift=1.328)\n",
      "Diagnosis_F41.9=1 ⇒ Service_Electrocardiogram (supp=0.015, conf=0.205, lift=1.760)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Configuration \n",
    "MIN_SUPPORT = 0.01          # min fraction of samples for an itemset\n",
    "MIN_CONFIDENCE = 0.10       \n",
    "TOP_K_RULES = 100             # rules to print per label\n",
    "MAX_ANTECEDENT = 2        \n",
    "MAX_BINS = 3              \n",
    "MIN_UNIQUE_NUMERIC = 5      # treat as numeric (else binary-like)\n",
    "MIN_ITEM_FREQ = 0.01        # drop very rare feature items \n",
    "\n",
    "\n",
    "# Combine all X and Y since we don't need train/val splits\n",
    "\n",
    "X_ARM = pd.concat([X_train_filtered, X_val_filtered])\n",
    "Y_ARM = pd.concat([y_train_filtered, y_val_filtered])\n",
    "\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "X_ARM = pd.DataFrame(X_ARM) if not isinstance(X_ARM, pd.DataFrame) else X_ARM\n",
    "Y_ARM = pd.DataFrame(Y_ARM) if not isinstance(Y_ARM, pd.DataFrame) else Y_ARM\n",
    "\n",
    "label_names = list(Y_ARM.columns)\n",
    "\n",
    "\n",
    "# Binarize X into boolean \"items\" -- either through bins or one-hot encoding\n",
    "def binarize_X_for_arm(X, max_bins=3, min_unique=5, min_item_freq=0.01):\n",
    "    items = {}\n",
    "    for col in X.columns:\n",
    "        s = X[col]\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            # Binary-like numeric\n",
    "            non_na = s.dropna()\n",
    "            if non_na.nunique() <= 2 or len(non_na) < 10:\n",
    "                # Treat non-zero as True\n",
    "                mask = (s.fillna(0) != 0)\n",
    "                if mask.mean() >= min_item_freq:\n",
    "                    items[f\"X::{col}=1\"] = mask.astype(bool)\n",
    "            else:\n",
    "                # Discretize into quantile bins\n",
    "                try:\n",
    "                    bins = min(max_bins, int(np.clip(non_na.nunique(), 2, max_bins)))\n",
    "                    cats = pd.qcut(s, q=bins, duplicates='drop')\n",
    "                    dummies = pd.get_dummies(cats, prefix=f\"X::{col}\", dummy_na=False)\n",
    "                    for dcol in dummies.columns:\n",
    "                        mask = dummies[dcol].astype(bool)\n",
    "                        if mask.mean() >= min_item_freq:\n",
    "                            items[str(dcol)] = mask\n",
    "                except Exception:\n",
    "                    # Fallback median split\n",
    "                    thr = float(non_na.median())\n",
    "                    le_mask = (s <= thr).fillna(False)\n",
    "                    gt_mask = (s > thr).fillna(False)\n",
    "                    if le_mask.mean() >= min_item_freq:\n",
    "                        items[f\"X::{col}<= {thr:.4g}\"] = le_mask\n",
    "                    if gt_mask.mean() >= min_item_freq:\n",
    "                        items[f\"X::{col}> {thr:.4g}\"] = gt_mask\n",
    "        else:\n",
    "            # Categorical -> one-hot\n",
    "            cats = s.astype('category')\n",
    "            dummies = pd.get_dummies(cats, prefix=f\"X::{col}\", dummy_na=False)\n",
    "            for dcol in dummies.columns:\n",
    "                mask = dummies[dcol].astype(bool)\n",
    "                if mask.mean() >= min_item_freq:\n",
    "                    items[str(dcol)] = mask\n",
    "    if not items:\n",
    "        return pd.DataFrame(index=X.index)\n",
    "    return pd.DataFrame(items, index=X.index).astype(bool)\n",
    "\n",
    "# Binarize Y (labels) into boolean columns with Y:: prefix \n",
    "def binarize_Y_for_arm(Y):\n",
    "    Y_bool = Y.astype(bool)\n",
    "    Y_bool.columns = [f\"Y::{c}\" for c in Y_bool.columns]\n",
    "    return Y_bool.astype(bool)\n",
    "\n",
    "X_items = binarize_X_for_arm(X_ARM, max_bins=MAX_BINS, min_unique=MIN_UNIQUE_NUMERIC, min_item_freq=MIN_ITEM_FREQ)\n",
    "Y_items = binarize_Y_for_arm(Y_ARM)\n",
    "\n",
    "if X_items.shape[1] == 0:\n",
    "    print(\"[ARM] No feature items were created (all too rare or non-informative). Try lowering MIN_ITEM_FREQ or adjust binning.\")\n",
    "    rules_xy = pd.DataFrame(columns=[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"])\n",
    "else:\n",
    "\n",
    "\n",
    "    # Combine X and Y items\n",
    "    XY = pd.concat([X_items, Y_items], axis=1)\n",
    "    # Frequent itemsets\n",
    "    try:\n",
    "        freq = fpgrowth(XY, min_support=MIN_SUPPORT, use_colnames=True)\n",
    "    except Exception:\n",
    "        freq = apriori(XY, min_support=MIN_SUPPORT, use_colnames=True)\n",
    "    # Calculate rules\n",
    "    if freq.empty:\n",
    "        rules_xy = pd.DataFrame(columns=[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"])\n",
    "    else:\n",
    "        rules = association_rules(freq, metric=\"confidence\", min_threshold=MIN_CONFIDENCE)\n",
    "        if rules.empty:\n",
    "            rules_xy = pd.DataFrame(columns=[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"])\n",
    "        else:\n",
    "            # Convert frozensets, filter: consequents are one Y label; antecedents only X items\n",
    "            def to_set(fs): return set(map(str, fs))\n",
    "            rules[\"antecedents\"] = rules[\"antecedents\"].apply(to_set)\n",
    "            rules[\"consequents\"] = rules[\"consequents\"].apply(to_set)\n",
    "            rules = rules[\n",
    "                (rules[\"consequents\"].apply(len) == 1) &\n",
    "                (rules[\"consequents\"].apply(lambda s: all(x.startswith(\"Y::\") for x in s))) &\n",
    "                (rules[\"antecedents\"].apply(len) >= 1) &\n",
    "                (rules[\"antecedents\"].apply(len) <= MAX_ANTECEDENT) &\n",
    "                (rules[\"antecedents\"].apply(lambda s: all(x.startswith(\"X::\") for x in s)))\n",
    "            ]\n",
    "            if rules.empty:\n",
    "                rules_xy = pd.DataFrame(columns=[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"])\n",
    "            else:\n",
    "                keep = [c for c in [\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\", \"leverage\", \"conviction\"] if c in rules.columns]\n",
    "                rules_xy = rules[keep].sort_values(\n",
    "                    by=[\"confidence\", \"lift\", \"support\"], ascending=[False, False, False]\n",
    "                ).reset_index(drop=True)\n",
    "    \n",
    "# Formatting helpers\n",
    "def _strip_prefix(name):\n",
    "    # remove \"X::\" or \"Y::\" for readability\n",
    "    if name.startswith(\"X::\"):\n",
    "        return name[3:]\n",
    "    if name.startswith(\"Y::\"):\n",
    "        return name[3:]\n",
    "    return name\n",
    "\n",
    "def format_rules_for_label_xy(rules_df, label_name, top_k=100):\n",
    "    \"\"\"Return strings like: 'feat_bin1 & feat_bin2 ⇒ label (supp=..., conf=..., lift=...)'\"\"\"\n",
    "    if rules_df is None or rules_df.empty:\n",
    "        return [\"(no rules found)\"]\n",
    "    y_token = f\"Y::{label_name}\"\n",
    "    mask = rules_df[\"consequents\"].apply(lambda s: s == {y_token})\n",
    "    sub = rules_df[mask]\n",
    "    if sub.empty:\n",
    "        return [\"(no rules found)\"]\n",
    "    lines = []\n",
    "    for _, row in sub.head(top_k).iterrows():\n",
    "        ants = \" & \".join(sorted([_strip_prefix(a) for a in row[\"antecedents\"]]))\n",
    "        cons = _strip_prefix(next(iter(row[\"consequents\"])))\n",
    "        lines.append(f\"{ants} ⇒ {cons} (supp={row['support']:.3f}, conf={row['confidence']:.3f}, lift={row.get('lift', np.nan):.3f})\")\n",
    "    return lines\n",
    "\n",
    "# Plain-text per-label output \n",
    "print(\"\\n=====================\")\n",
    "print(\"X → Y Association Rules (confidence ≥ 0.10)\")\n",
    "print(\"=====================\")\n",
    "for label in label_names:\n",
    "    print(f\"\\n=== Target: {label} ===\")\n",
    "    lines = format_rules_for_label_xy(rules_xy, label, top_k=TOP_K_RULES)\n",
    "    for line in lines:\n",
    "        print(f\"  - {line}\")\n",
    "\n",
    "# global snapshot\n",
    "def print_global_rules_snapshot(rules_df, title, k=100):\n",
    "    print(f\"\\n--- {title} (top {k}) ---\")\n",
    "    if rules_df is None or rules_df.empty:\n",
    "        print(\"(no rules found)\")\n",
    "        return\n",
    "    sub = rules_df.head(k)\n",
    "    for _, row in sub.iterrows():\n",
    "        ants = \" & \".join(sorted([_strip_prefix(a) for a in row[\"antecedents\"]]))\n",
    "        cons = \" & \".join(sorted([_strip_prefix(c) for c in row[\"consequents\"]]))\n",
    "        print(f\"{ants} ⇒ {cons} (supp={row['support']:.3f}, conf={row['confidence']:.3f}, lift={row.get('lift', np.nan):.3f})\")\n",
    "\n",
    "print_global_rules_snapshot(rules_xy, \"Global X → Y rules\", k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed0d469-02df-4310-a6ed-4e415c85ac17",
   "metadata": {},
   "source": [
    "The first rule, Diagnosis_K57.30, indicates that members with \"Diverticulosis of large intestine without perforation or abscess without bleeding\" are more likely to get a colonoscopy than other members. This potentially indicates that members who have this diagnosis should look into getting a colonoscopy if they have not done so already"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d33d20-b701-41ba-a673-0f0d06f62441",
   "metadata": {},
   "source": [
    "## Importance scores on specific services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "36a73f8d-14fb-4f49-9779-9f6f186c8794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 important features for predicting Service_Gallbladder_surgery\n",
      "               feature  importance\n",
      "239   Diagnosis_K80.10    0.157337\n",
      "240   Diagnosis_K80.20    0.116253\n",
      "49     Diagnosis_D25.9    0.047053\n",
      "236    Diagnosis_K76.0    0.042865\n",
      "370  Diagnosis_Z79.899    0.041298\n",
      "219    Diagnosis_K42.9    0.038930\n",
      "85     Diagnosis_E78.2    0.038891\n",
      "214    Diagnosis_K21.9    0.037684\n",
      "168    Diagnosis_H52.4    0.037038\n",
      "173      Diagnosis_I10    0.036944\n",
      "407   Diagnosis_Z90.49    0.036007\n",
      "83    Diagnosis_E78.00    0.035318\n",
      "119    Diagnosis_F41.1    0.034203\n",
      "349    Diagnosis_N92.0    0.034005\n",
      "158  Diagnosis_H04.123    0.033700\n",
      "54     Diagnosis_D50.9    0.030841\n",
      "87     Diagnosis_E78.5    0.030042\n",
      "1      Diagnosis_238.2    0.029270\n",
      "147   Diagnosis_G47.00    0.029254\n",
      "78    Diagnosis_E66.01    0.029149\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assume the target name and index\n",
    "target_name = \"Service_Gallbladder_surgery\"\n",
    "\n",
    "# Find the index of the target in y_val_filtered\n",
    "target_index = y_val_filtered.columns.get_loc(target_name)\n",
    "\n",
    "# Get the corresponding model\n",
    "model = loaded_models[target_index]\n",
    "\n",
    "# Get feature names from X_val_filtered\n",
    "feature_names = X_val_filtered.columns\n",
    "\n",
    "# Determine feature importance based on model type\n",
    "if hasattr(model, \"coef_\"):\n",
    "    # For linear models (e.g., Logistic Regression)\n",
    "    importance = model.coef_.ravel()\n",
    "elif hasattr(model, \"feature_importances_\"):\n",
    "    # For tree-based models (e.g., RandomForest, XGBoost)\n",
    "    importance = model.feature_importances_\n",
    "else:\n",
    "    raise ValueError(\"Model does not support feature importance extraction.\")\n",
    "\n",
    "# Create a DataFrame of feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": importance\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "# Display top 20 most important features\n",
    "print(\"Top 20 important features for predicting\", target_name)\n",
    "print(importance_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44af66a7-af5e-485f-9157-5d4afbdba7ab",
   "metadata": {},
   "source": [
    "This indicates that members with gallstones are more likely to get gallbladder surgery, and if they have not received one, should likely be warned about it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
